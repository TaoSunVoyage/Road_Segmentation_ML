{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "U-Net_3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "1bgDY-_4fSH8",
        "colab_type": "code",
        "outputId": "0c588090-fcf0-4245-8a58-7db837661025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UWnI4AylsDG7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_path = \"drive/My Drive/training\"\n",
        "val_path = \"drive/My Drive/validation\"\n",
        "test_path = \"drive/My Drive/test_set_images\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6uxgtKm6hwU",
        "colab_type": "code",
        "outputId": "c182653c-296b-4c8a-ce01-51b893a689e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "import random\n",
        "random.seed(1)\n",
        "\n",
        "import os\n",
        "import skimage.io as io\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.losses import *\n",
        "from keras.optimizers import *\n",
        "import keras.backend as K\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, TensorBoard\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "rOEJTVugwLfr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Enrichment and Augmentation\n",
        "Enrich our trainning set:\n",
        "1.   Filip original image\n",
        "2.   Rotate image and filpped version by {90, 180, 270} degree\n",
        "\n",
        "In this way we get eight-fold our original data set and we consider this enriched dataset as **real** data. It makes sence as in real world satellite images may also be shown in different ways, e.g. upside-down, or horizontally mirrored. \n",
        "\n",
        "After this step, we preserve 10% of enriched dataset as validation set and remain as training set.\n",
        "\n",
        "During training, we perform stochastic data augmentaion on training set; for validation, we only use the 10% data without any augmentation.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "gqSsjUfL9OUn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Enrichment"
      ]
    },
    {
      "metadata": {
        "id": "-EzAHF-3OhR0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Data Enrichment - flip & rotate\n",
        "\n",
        "for i in range(1, 101):\n",
        "    im = Image.open(os.path.join(train_path, 'images', 'satImage_%.3d.png'%i))\n",
        "    ma = Image.open(os.path.join(train_path, 'groundtruth', 'satImage_%.3d.png'%i))\n",
        "\n",
        "    im_f = im.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    io.imsave(os.path.join(train_path, 'images', 'satImage_%.3d_f.png'%i), im_f)\n",
        "\n",
        "    ma_f = ma.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    io.imsave(os.path.join(train_path, 'groundtruth', 'satImage_%.3d_f.png'%i), ma_f)\n",
        "  \n",
        "  \n",
        "    for angle in [90, 180, 270]:\n",
        "        im_r = im.rotate(angle)\n",
        "        io.imsave(os.path.join(train_path, 'images', 'satImage_%.3d_%.3d.png'%(i, angle)), im_r)\n",
        "\n",
        "        im_f_r = im_f.rotate(angle)\n",
        "        io.imsave(os.path.join(train_path, 'images', 'satImage_%.3d_f_%.3d.png'%(i, angle)), im_f_r)\n",
        "\n",
        "        ma_r = ma.rotate(angle)\n",
        "        io.imsave(os.path.join(train_path, 'groundtruth', 'satImage_%.3d_%.3d.png'%(i, angle)), ma_r)\n",
        "\n",
        "        ma_f_r = ma_f.rotate(angle)\n",
        "        io.imsave(os.path.join(train_path, 'groundtruth', 'satImage_%.3d_f_%.3d.png'%(i, angle)), ma_f_r)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rWKVdD_B5e6G",
        "colab_type": "code",
        "outputId": "38c700f4-d92c-44b0-d250-91fcdcbd49c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_val_images = os.listdir(os.path.join(train_path, 'images'))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_images, val_images = train_test_split(train_val_images, test_size=0.2, random_state=1)\n",
        "len(train_images), len(val_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(640, 160)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "66PbDToq6u1E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir \"/content/drive/My Drive/validation\"\n",
        "!mkdir \"/content/drive/My Drive/validation/images\"\n",
        "!mkdir \"/content/drive/My Drive/validation/groundtruth\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KboYfCSN8idR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "val_path = \"/content/drive/My Drive/validation\"\n",
        "for im in val_images:\n",
        "    os.rename(os.path.join(train_path, 'images', im), os.path.join(val_path, 'images', im))\n",
        "    os.rename(os.path.join(train_path, 'groundtruth', im), os.path.join(val_path, 'groundtruth', im))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C11U0KjY9K0T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "metadata": {
        "id": "7wUP313czr13",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_mask(mask):\n",
        "    mask = mask/255\n",
        "    mask[mask > 0.5] = 1\n",
        "    mask[mask <= 0.5] = 0\n",
        "    return mask\n",
        "  \n",
        "def preprocess_img(img):\n",
        "    return img/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_0OLI52zsnp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainvalGenerator(batch_size, train_path, val_path,\n",
        "                      image_folder, mask_folder,\n",
        "                      aug_dict, train_dir = None, val_dir = None,\n",
        "                      target_size = (400,400), seed = 1):\n",
        "    '''\n",
        "    can generate image and mask at the same time\n",
        "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
        "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    # Train\n",
        "    image_dict = aug_dict.copy()\n",
        "    image_dict[\"preprocessing_function\"] = preprocess_img\n",
        "    image_datagen = ImageDataGenerator(**image_dict)\n",
        "    \n",
        "    mask_dict = aug_dict.copy()\n",
        "    mask_dict[\"preprocessing_function\"] = preprocess_mask\n",
        "    mask_datagen = ImageDataGenerator(**mask_dict)\n",
        "    \n",
        "    image_generator_train = image_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [image_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = \"rgb\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = train_dir,\n",
        "        save_prefix  = \"image\",\n",
        "        seed = seed)\n",
        "    \n",
        "    mask_generator_train = mask_datagen.flow_from_directory(\n",
        "        train_path,\n",
        "        classes = [mask_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = train_dir,\n",
        "        save_prefix  = \"mask\",\n",
        "        seed = seed)\n",
        "    \n",
        "    # Validation\n",
        "    image_datagen = ImageDataGenerator(preprocessing_function=preprocess_img)\n",
        "    mask_datagen = ImageDataGenerator(preprocessing_function=preprocess_mask)\n",
        "    \n",
        "    image_generator_val = image_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        classes = [image_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = \"rgb\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = val_dir,\n",
        "        save_prefix  = \"image\",\n",
        "        seed = seed+1,\n",
        "        shuffle = False\n",
        "    )\n",
        "    \n",
        "    mask_generator_val = mask_datagen.flow_from_directory(\n",
        "        val_path,\n",
        "        classes = [mask_folder],\n",
        "        class_mode = None,\n",
        "        color_mode = \"grayscale\",\n",
        "        target_size = target_size,\n",
        "        batch_size = batch_size,\n",
        "        save_to_dir = val_dir,\n",
        "        save_prefix  = \"mask\",\n",
        "        seed = seed+1,\n",
        "        shuffle = False\n",
        "    )\n",
        "    \n",
        "    return zip(image_generator_train, mask_generator_train), zip(image_generator_val, mask_generator_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bCuvrDlPzzEK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# argument for data augmentation\n",
        "data_gen_args = dict(rotation_range=45,\n",
        "                     width_shift_range=0.1,\n",
        "                     height_shift_range=0.1,\n",
        "                     horizontal_flip=True,\n",
        "                     vertical_flip=True,\n",
        "                     fill_mode='reflect')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frTWFcvPz20j",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf train_image\n",
        "!rm -rf val_image\n",
        "!mkdir train_image\n",
        "!mkdir val_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z5gSWJd2z2Hs",
        "colab_type": "code",
        "outputId": "0a030543-271b-4738-b54c-fba750fcf597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "trainGen, valGen = trainvalGenerator(batch_size=2, \n",
        "                                     train_path=train_path, val_path=val_path,\n",
        "                                     image_folder='images', mask_folder='groundtruth',\n",
        "                                     aug_dict=data_gen_args, \n",
        "                                     train_dir = \"train_image\", # Set it to None if you don't want to save\n",
        "                                     val_dir = None, # Set it to None if you don't want to save\n",
        "                                     target_size = (400, 400), seed = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 640 images belonging to 1 classes.\n",
            "Found 640 images belonging to 1 classes.\n",
            "Found 160 images belonging to 1 classes.\n",
            "Found 160 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UBmHBak0z-vo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Metrics and Loss"
      ]
    },
    {
      "metadata": {
        "id": "oA3rwAgMr_Nj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# credits: https://stackoverflow.com/questions/43547402/how-to-calculate-f1-macro-in-keras\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        \"\"\"Recall metric.\n",
        "\n",
        "        Only computes a batch-wise average of recall.\n",
        "\n",
        "        Computes the recall, a metric for multi-label classification of\n",
        "        how many relevant items are selected.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        \"\"\"Precision metric.\n",
        "\n",
        "        Only computes a batch-wise average of precision.\n",
        "\n",
        "        Computes the precision, a metric for multi-label classification of\n",
        "        how many selected items are relevant.\n",
        "        \"\"\"\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DslNaQxLuHzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# https://arxiv.org/pdf/1606.04797.pdf\n",
        "# https://arxiv.org/pdf/1707.03237.pdf\n",
        "\n",
        "# credits: https://github.com/petrosgk/Kaggle-Carvana-Image-Masking-Challenge/blob/master/model/losses.py\n",
        "from keras.losses import binary_crossentropy\n",
        "import keras.backend as K\n",
        "\n",
        "\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    smooth = 1.\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return score\n",
        "\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def bce_dice_loss(y_true, y_pred):\n",
        "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U57_hYj3k1qp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# foreground_threshold = 0.25\n",
        "\n",
        "# def patch_to_label(patch):\n",
        "#     df = np.mean(patch)\n",
        "#     if df > foreground_threshold:\n",
        "#         return 1\n",
        "#     else:\n",
        "#         return 0\n",
        "      \n",
        "# def get_label(y, patch_size=16):\n",
        "#     label = []\n",
        "#     y_array = K.eval(y[:,:,:,0])\n",
        "#     for im in y_array:\n",
        "#       for j in range(0, im.shape[1], patch_size):\n",
        "#         for i in range(0, im.shape[0], patch_size):\n",
        "#             patch = im[i:i + patch_size, j:j + patch_size]\n",
        "#             label.append(patch_to_label(patch))\n",
        "#     return np.array(label)\n",
        "  \n",
        "# def patch_f1(y_true, y_pred):\n",
        "#     true_label = get_label(y_true)\n",
        "#     pred_label = get_label(y_pred)\n",
        "#     return f1(true_label, pred_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ykKFQvakr_Np",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # credits: https://www.kaggle.com/guglielmocamporese/macro-f1-score-keras\n",
        "\n",
        "# def f1(y_true, y_pred):\n",
        "#     #y_pred = K.round(y_pred)\n",
        "#     #y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), THRESHOLD), K.floatx())\n",
        "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "#     p = tp / (tp + fp + K.epsilon())\n",
        "#     r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
        "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "#     return K.mean(f1)\n",
        "  \n",
        "# def f1_loss(y_true, y_pred):\n",
        "    \n",
        "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "#     tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "#     p = tp / (tp + fp + K.epsilon())\n",
        "#     r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
        "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "#     return 1 - K.mean(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DOLzXLa0GJ0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Network"
      ]
    },
    {
      "metadata": {
        "id": "l7Gu5oZgCgie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def conv2d_block(input_tensor, n_filter, kernel_size=3, batchnorm=True, activation='relu'):\n",
        "    # first layer\n",
        "    x = Conv2D(n_filter, kernel_size=kernel_size, kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(input_tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "    # second layer\n",
        "    x = Conv2D(n_filter, kernel_size=kernel_size, kernel_initializer=\"he_normal\",\n",
        "               padding=\"same\")(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2yp2xDTpr_Nv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def unet(pretrained_weights = None,\n",
        "         input_size = (None,None,3),\n",
        "         n_filter=16,\n",
        "         activation='relu',\n",
        "         dropout=True, dropout_rate=0.5,\n",
        "         batchnorm=True,\n",
        "         loss=binary_crossentropy,\n",
        "         optimizer=Adam(lr=1e-4)\n",
        "        ):\n",
        "  \n",
        "    # 3\n",
        "    inputs = Input(input_size)\n",
        "    \n",
        "    # down path\n",
        "    # n_filter\n",
        "    conv1 = conv2d_block(inputs, n_filter, kernel_size=3, batchnorm=batchnorm, activation=activation)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    \n",
        "    # n_filter*2\n",
        "    conv2 = conv2d_block(pool1, n_filter*2, kernel_size=3, batchnorm=batchnorm, activation=activation)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    \n",
        "    # n_filter*4\n",
        "    conv3 = conv2d_block(pool2, n_filter*4, kernel_size=3, batchnorm=batchnorm, activation=activation)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    \n",
        "    # n_filter*8\n",
        "    conv4 = conv2d_block(pool3, n_filter*8, kernel_size=3, batchnorm=batchnorm, activation=activation)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "\n",
        "    # central path\n",
        "    # n_filter*16\n",
        "    conv5 = conv2d_block(pool4, n_filter*16, kernel_size=3, batchnorm=batchnorm, activation=activation)\n",
        "\n",
        "    # up path\n",
        "    # n_filter*8\n",
        "    up6 = Conv2DTranspose(n_filter*8, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(conv5)\n",
        "    merge6 = concatenate([conv4, up6], axis = 3)\n",
        "    merge6 = Dropout(dropout_rate)(merge6) if dropout else merge6\n",
        "    conv6 = conv2d_block(merge6, n_filter*8, kernel_size=3, batchnorm=False, activation=activation)\n",
        "    \n",
        "    # n_filter*4\n",
        "    up7 = Conv2DTranspose(n_filter*4, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(conv6)\n",
        "    merge7 = concatenate([conv3, up7], axis = 3)\n",
        "    merge7 = Dropout(dropout_rate)(merge7) if dropout else merge7\n",
        "    conv7 = conv2d_block(merge7, n_filter*4, kernel_size=3, batchnorm=False, activation=activation)\n",
        "    \n",
        "    # n_filter*2\n",
        "    up8 = Conv2DTranspose(n_filter*2, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(conv7)\n",
        "    merge8 = concatenate([conv2, up8], axis = 3)\n",
        "    merge8 = Dropout(dropout_rate)(merge8) if dropout else merge8\n",
        "    conv8 = conv2d_block(merge8, n_filter*2, kernel_size=3, batchnorm=False, activation=activation)\n",
        "    \n",
        "    # n_filter\n",
        "    up9 = Conv2DTranspose(n_filter, kernel_size=2, strides=2, kernel_initializer=\"he_normal\", padding='same')(conv8)\n",
        "    merge9 = concatenate([conv1, up9], axis = 3)\n",
        "    merge9 = Dropout(dropout_rate)(merge9) if dropout else merge9\n",
        "    conv9 = conv2d_block(merge9, n_filter, kernel_size=3, batchnorm=False, activation=activation)\n",
        "    \n",
        "    # classifier\n",
        "    conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = optimizer, loss = loss, metrics = [f1, 'accuracy'])\n",
        "    \n",
        "    if(pretrained_weights):\n",
        "        model.load_weights(filepath=pretrained_weights)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knx3uNw6r_OB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = unet(pretrained_weights='drive/My Drive/weights_64_0.2_elu_dice.h5', n_filter=64, activation='elu', dropout_rate=0.2, loss=dice_loss, optimizer=Adam(lr=1e-6))\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GB1nBuWH0I7z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train"
      ]
    },
    {
      "metadata": {
        "id": "5ReAr6s7r_OG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "#     ModelCheckpoint('weights_no_val.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    EarlyStopping(monitor='val_loss', patience=9, verbose=1, min_delta=1e-4),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4),\n",
        "    ModelCheckpoint('drive/My Drive/weights_64_0.2_elu_dice_train_more.h5', monitor='val_loss', save_best_only=True, verbose=1),\n",
        "    TensorBoard(log_dir='tensorboard/', write_graph=True, write_images=True)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pokgC4X9r_OX",
        "colab_type": "code",
        "outputId": "9d99eb2b-bb5f-4ec6-c2ea-8a35d0f51a75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2394
        }
      },
      "cell_type": "code",
      "source": [
        "print('*'*30)\n",
        "print('Fitting model...')\n",
        "print('*'*30)\n",
        "history = model.fit_generator(generator=trainGen, steps_per_epoch=100,\n",
        "                              validation_data=valGen, validation_steps=80,\n",
        "                              epochs=100, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************************\n",
            "Fitting model...\n",
            "******************************\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 112s 1s/step - loss: 0.1014 - f1: 0.8986 - acc: 0.9579 - val_loss: 0.1128 - val_f1: 0.8872 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00001: val_loss did not improve from 0.11230\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0903 - f1: 0.9097 - acc: 0.9647 - val_loss: 0.1127 - val_f1: 0.8874 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.11230\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0840 - f1: 0.9160 - acc: 0.9667 - val_loss: 0.1128 - val_f1: 0.8872 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.11230\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0862 - f1: 0.9138 - acc: 0.9666 - val_loss: 0.1126 - val_f1: 0.8874 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.11230\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0907 - f1: 0.9093 - acc: 0.9655 - val_loss: 0.1128 - val_f1: 0.8873 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00005: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.11230\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0918 - f1: 0.9083 - acc: 0.9653 - val_loss: 0.1127 - val_f1: 0.8873 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.11230\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0929 - f1: 0.9071 - acc: 0.9630 - val_loss: 0.1125 - val_f1: 0.8875 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.11230\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0834 - f1: 0.9166 - acc: 0.9665 - val_loss: 0.1129 - val_f1: 0.8872 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.11230\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0927 - f1: 0.9073 - acc: 0.9636 - val_loss: 0.1123 - val_f1: 0.8877 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.11230\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0895 - f1: 0.9105 - acc: 0.9656 - val_loss: 0.1125 - val_f1: 0.8876 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.11230\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0882 - f1: 0.9119 - acc: 0.9652 - val_loss: 0.1128 - val_f1: 0.8872 - val_acc: 0.9572\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.11230\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0887 - f1: 0.9113 - acc: 0.9657 - val_loss: 0.1129 - val_f1: 0.8871 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 9.999999939225292e-10.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.11230\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0867 - f1: 0.9134 - acc: 0.9656 - val_loss: 0.1130 - val_f1: 0.8871 - val_acc: 0.9571\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.11230\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0906 - f1: 0.9094 - acc: 0.9642 - val_loss: 0.1128 - val_f1: 0.8872 - val_acc: 0.9569\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.11230\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0913 - f1: 0.9088 - acc: 0.9653 - val_loss: 0.1123 - val_f1: 0.8877 - val_acc: 0.9572\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 9.999999717180686e-11.\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.11230 to 0.11227, saving model to drive/My Drive/weights_64_0.2_elu_dice_train_more.h5\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0869 - f1: 0.9131 - acc: 0.9668 - val_loss: 0.1123 - val_f1: 0.8877 - val_acc: 0.9572\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.11227\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 111s 1s/step - loss: 0.0846 - f1: 0.9155 - acc: 0.9665 - val_loss: 0.1128 - val_f1: 0.8872 - val_acc: 0.9570\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.11227\n",
            "Epoch 18/100\n",
            " 42/100 [===========>..................] - ETA: 49s - loss: 0.0962 - f1: 0.9038 - acc: 0.9651"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5823c353b8fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m history = model.fit_generator(generator=trainGen, steps_per_epoch=100,\n\u001b[1;32m      5\u001b[0m                               \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalGen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                               epochs=100, callbacks=callbacks)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "PT7H62qNCvMZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp weights_32_0.2_elu_dice_train_more.h5 \"drive/My Drive\"\n",
        "# # !cp weights_32_drop_relu.h5 \"drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M8pYyakN0LqR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Predict"
      ]
    },
    {
      "metadata": {
        "id": "BsGVNX1nr_Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import skimage.io as io\n",
        "def testGenerator(test_path, num_image = 50):\n",
        "    for i in range(1,num_image+1):\n",
        "        img = io.imread(os.path.join(test_path, \"test_%d\"%i, \"test_%d.png\"%i))\n",
        "        img = img / 255\n",
        "        img = np.reshape(img,(1,)+img.shape)\n",
        "        yield img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noWhzXJ3r_Od",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testGene = testGenerator(test_path)\n",
        "# model_predict = unet(pretrained_weights = \"weights_no_val_32.h5\", input_size = (608,608,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9hZnm9T18vEM",
        "colab_type": "code",
        "outputId": "b4891636-9e96-4fc3-87d6-beeb83840d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "result = model.predict_generator(testGene, 50, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 39s 779ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EdSPM5qRr_Oj",
        "colab_type": "code",
        "outputId": "5b9a3b69-acbc-4ce3-b70a-0396b7730237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "def saveResult(save_path, npyfile):\n",
        "    for i,item in enumerate(npyfile):\n",
        "        img = item[:,:,0]\n",
        "        io.imsave(os.path.join(save_path, \"%d_predict.png\"%(i+1)), img)\n",
        "\n",
        "!rm -rf test\n",
        "!mkdir test        \n",
        "saveResult(\"test\", result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float32 to uint16\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lWlLqIgRr_Ol",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.image as mpimg\n",
        "import re\n",
        "\n",
        "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
        "\n",
        "# assign a label to a patch\n",
        "def patch_to_label(patch):\n",
        "    df = np.mean(patch)\n",
        "    if df > foreground_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def mask_to_submission_strings(image_filename):\n",
        "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
        "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
        "    im = mpimg.imread(image_filename)\n",
        "    patch_size = 16\n",
        "    for j in range(0, im.shape[1], patch_size):\n",
        "        for i in range(0, im.shape[0], patch_size):\n",
        "            patch = im[i:i + patch_size, j:j + patch_size]\n",
        "            label = patch_to_label(patch)\n",
        "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
        "\n",
        "\n",
        "def masks_to_submission(submission_filename, *image_filenames):\n",
        "    \"\"\"Converts images into a submission file\"\"\"\n",
        "    with open(submission_filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for fn in image_filenames[0:]:\n",
        "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q6u0lzjwr_On",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission_filename = 'submission_64_0.2_elu_dice_train_more.csv'\n",
        "image_filenames = []\n",
        "predict_path = 'test/'\n",
        "for i in range(1, 51):\n",
        "    image_filename = predict_path + '%d' % i + '_predict.png'\n",
        "#     print(image_filename)\n",
        "    image_filenames.append(image_filename)\n",
        "masks_to_submission(submission_filename, *image_filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXd0XEiwTpJ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp submission_64_0.2_elu_dice_train_more.csv \"drive/My Drive\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HAYAadmUqy8S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Postprocessing"
      ]
    },
    {
      "metadata": {
        "id": "TWjPjN03nbCZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from skimage.morphology import "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Q2xqTIxnjOC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "im = plt.imread(os.path.join(\"test\", \"%d_predict.png\"%20))\n",
        "im_c = opening(im, square(8))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c8bTJLnvnqdK",
        "colab_type": "code",
        "outputId": "ca483287-fa00-4378-9c85-b72dae04c493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(im_c, 'gray')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ff45adbba20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAHVCAYAAABbrEo9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nriRJOTBwJUnKwf8D7kAQNbpHwXMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff45c487400>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "R6bqSWCCHcGm",
        "colab_type": "code",
        "outputId": "ba44b27f-7065-4a5b-f015-ef43aaa22504",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir test_open\n",
        "for i in range(1, 51):\n",
        "    im = plt.imread(os.path.join(\"test\", \"%d_predict.png\"%i))\n",
        "    im_c = opening(im, square(8))\n",
        "    io.imsave(os.path.join(\"test_open\", \"%d_predict.png\"%i), im_c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/skimage/util/dtype.py:122: UserWarning: Possible precision loss when converting from float32 to uint16\n",
            "  .format(dtypeobj_in, dtypeobj_out))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "qwPTq6EjqfqA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "submission_filename = 'submission_64_0.2_elu_dice_train_more_open8.csv'\n",
        "image_filenames = []\n",
        "predict_path = 'test_open/'\n",
        "for i in range(1, 51):\n",
        "    image_filename = predict_path + '%d' % i + '_predict.png'\n",
        "#     print(image_filename)\n",
        "    image_filenames.append(image_filename)\n",
        "masks_to_submission(submission_filename, *image_filenames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jwrp8zxtqkhY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}